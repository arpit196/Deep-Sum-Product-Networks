# -*- coding: utf-8 -*-
"""SumProductNetworks.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bFFjMNNZjQWCY9YDcQTGnBz8OPYdohTM
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import keras

from sklearn.model_selection import KFold
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

from collections import Counter

from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve
sns.set(style='white', context='notebook', palette='deep')

from sklearn.model_selection import KFold
kf = KFold(n_splits=10, random_state=None, shuffle=False)
kfold = KFold(n_splits=10, random_state=None, shuffle=False)

# Load data
##### Load the dataset for PIMA diabetes

df = pd.read_csv("../input/pima-indians-diabetes-database/diabetes.csv")
def label_encoder(dataframe, binary_col):
    labelencoder = LabelEncoder()
    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])
    return dataframe

binary_cols = [col for col in df.columns if df[col].dtype not in [int, float] and df[col].nunique() == 2]
len(binary_cols)

for col in binary_cols:
    label_encoder(df, col)

df.head()

from sklearn.model_selection import KFold
from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, RobustScaler
def label_encoder(dataframe, binary_col):
    labelencoder = LabelEncoder()
    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])
    return dataframe

binary_cols = 'class'
#len(binary_cols)
labelencoder = LabelEncoder()
for col in df.columns:
    df[col] = labelencoder.fit_transform(df[col])

label_encoder(df, binary_cols)

df.head()

def one_hot_encoder(dataframe, categorical_cols, drop_first=False):
    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)
    return dataframe

ohe_cols = [col for col in df.columns if 10 >= df[col].nunique() > 2]
df = one_hot_encoder(df, ohe_cols, drop_first=True)
df.head()

def grab_col_names(dataframe, cat_th=10, car_th=20):
    # cat_cols, cat_but_car
    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == "O"]
    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and
                   dataframe[col].dtypes != "O"]
    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and
                   dataframe[col].dtypes == "O"]
    cat_cols = cat_cols + num_but_cat
    cat_cols = [col for col in cat_cols if col not in cat_but_car]

    # num_cols
    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != "O"]
    num_cols = [col for col in num_cols if col not in num_but_cat]

    print(f"Observations: {dataframe.shape[0]}")
    print(f"Variables: {dataframe.shape[1]}")
    print(f'cat_cols: {len(cat_cols)}')
    print(f'num_cols: {len(num_cols)}')
    print(f'cat_but_car: {len(cat_but_car)}')
    print(f'num_but_cat: {len(num_but_cat)}')

    return cat_cols, num_cols, cat_but_car
cat_cols, num_cols, cat_but_car = grab_col_names(df)

df[num_cols] = scaler.fit_transform(df[num_cols])
import numpy as np
from sklearn.model_selection import KFold
from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, RobustScaler
y = df["Outcome"]   #Class label
X = df.drop(["Outcome"], axis=1)

from sklearn.model_selection import train_test_split, cross_validate

#Import different validation metrics
from sklearn.metrics import f1_score, roc_auc_score

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)

class SumProductLayer(keras.layers.Layer):
    def __init__(self, units=32, nlayers=1, input_dim=66):
        super().__init__()
        self.layers=nlayers
        self.w_init = tf.random_normal_initializer()
        self.w = tf.Variable(
            initial_value=self.w_init(shape=(units,input_dim), dtype="float32"),
            trainable=True,
        )
        self.weight = tf.Variable(
            initial_value=self.w_init(shape=(units,input_dim), dtype="float32"),
            trainable=True,
        )
        b_init = tf.zeros_initializer()
        self.b = tf.Variable(
            initial_value=b_init(shape=(units,input_dim,), dtype="float32"), trainable=True
        )
        self.bias = tf.Variable(
            initial_value=b_init(shape=(units,), dtype="float32"), trainable=True
        )
        self.kernel = self.add_weight("kernel",
                                  shape=[int(input_dim),units])
        self.nunits = units
        self.dense = layers.Dense(self.nunits,activation='sigmoid')

    def addweight(self,num_units,input_dim=66):
        self.nunits += 1
        self.newweight=tf.Variable(
            initial_value=self.w_init(shape=(num_units,input_dim), dtype="float32"),
            trainable=True,
        )
        self.weight = tf.concat([self.weight,self.newweight],axis=0)

    def call(self, inputs):
        #Passing the inputs through a linear layer
        out = tf.matmul(inputs,self.kernel)+self.bias
        out = tf.keras.activations.relu(out)
        #Taking pairwise product of the output of functions L-1 times to learn conjunctive AND like concepts with up to L-1 terms
        for layer in range(self.layers-1):
            out = tf.reduce_sum(out[:,:,tf.newaxis]*out[:,tf.newaxis,:],-1)
        return out

import tensorflow as tf
import keras
import keras.layers as layers
class SumProductNetwork(keras.Model):
    def __init__(self,size=32,layers=1,input_dim=66):
      super(AdditiveModel, self).__init__()
      self.size = size; self.layers=l


      inp = layers.Input(shape=(input_dim,))

      #Pass the input to a sum product layer
      out = SumProductLayer(units=size,input_dim=input_dim)(inp)

      #Pass through L layers of sum product networks along with a skip connection
      for l in range(self.layers-1):
        out = out + layers.Dense(out.shape[-1],activation='linear',kernel_regularizer=tf.keras.regularizers.L1(0.005))(SumProductLayer(units=size,nlayers=self.layers,input_dim=out.shape[-1])(out))

      #Softmax layer to classify the input into categories
      cls = layers.Dense(2,activation='softmax')(out)
      cls_model = keras.Model(inputs=inp,outputs=cls)
      self.classifier = cls_model

      #The output of the layer before the softmax layer is used as an encoder model
      self.encoder = keras.Model(inputs=inp,outputs=out)

      #Define the loss function and evaluation metrics
      self.cls_loss = tf.keras.losses.SparseCategoricalCrossentropy()
      self.acc = keras.metrics.SparseCategoricalAccuracy()#keras.metrics.Hinge()

    def compile(self, optimizer):
      super().compile(optimizer)
      self.optimizer = optimizer

    def call(self,inputs):
      return self.classifier(inputs)

    def train_step(self, data):
      X, label = data

      label = tf.cast(label,dtype=tf.float32)
      with tf.GradientTape(persistent=True) as tape:
          y_pred = self.classifier(X)
          loss = self.cls_loss(label,y_pred)
          pairwise = tf.subtract(label[:,np.newaxis],label[np.newaxis,:])
          pred_diff = tf.subtract(tf.transpose(y_pred)[:,:,np.newaxis],tf.transpose(y_pred)[:,np.newaxis,:])
          pred_diff = tf.reduce_mean(tf.transpose(tf.where(pred_diff<1.0,pred_diff,0),[1,2,0]),-1)
          same = tf.where(pairwise==0,pred_diff,pred_diff)  #Manifold loss to increase the pairwise distance between data from different classes
          same = tf.cast(same,tf.float32)
          diff = tf.reduce_mean(tf.where(pairwise==0,same**2,-pred_diff**2))
          acc = self.acc(label,y_pred)

      grads = tape.gradient(loss, self.classifier.trainable_variables)
      self.optimizer.apply_gradients(zip(grads, self.classifier.trainable_variables))
      grads = tape.gradient(diff, self.encoder.trainable_variables)
      self.optimizer.apply_gradients(zip(grads, self.encoder.trainable_variables))
      return {"contrastive_loss":loss,"acc":acc}

    def test_step(self, data):
      X, label = data
      y_pred = self.classifier(X)
      pairwise = tf.subtract(label[:,np.newaxis],label[np.newaxis,:])
      pred_diff = tf.subtract(tf.transpose(y_pred)[:,:,np.newaxis],tf.transpose(y_pred)[:,np.newaxis,:])
      pred_diff = tf.reduce_sum(tf.transpose(tf.where(pred_diff<1.0,pred_diff,0),[1,2,0]),-1)
      same = tf.where(pairwise==0,pred_diff,0.0)
      #y_pred = np.squeeze(y_pred)
      same = tf.cast(same,tf.float32)
      diff = tf.reduce_mean(tf.where(pairwise==0,same**2,-pred_diff**2))
      acc = self.acc(label,y_pred)
      return {"diff":diff,"val_acc": acc}

val_loss = 10
best_model = None

#Hyperparameter search using grid search
for layer in [1,2,3,4]:
  for units in [66,78,128]:

    m=AdditiveModel(size=units,l=layer)
    m.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001))
    m.fit(X_train,Y_train,validation_data=(X_train,Y_train),epochs=250,batch_size=200,verbose=0)
    kf = KFold(n_splits=10, random_state=None, shuffle=False)

    mean_score=0
    mean_loss=0
    for i, (train_index, test_index) in enumerate(kf.split(X_train)):
        X_tr = X_train.iloc[train_index]
        y_tr = Y_train.iloc[train_index]
        X_te = X_train.iloc[test_index]
        y_te = Y_train.iloc[test_index]
        print(m.evaluate(X_te,y_te,batch_size=100))
        mean_score += m.evaluate(X_te,y_te,batch_size=100)[0]
        mean_loss += m.evaluate(X_te,y_te,batch_size=100)[1]

    mean_score = mean_score/10
    mean_loss = mean_loss/10
    print(mean_score)
    print(mean_loss)
    if(mean_loss<val_loss):
        best_model = m
        val_loss=mean_loss